import logging
import argparse
from deita.pipeline import Pipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

parser = argparse.ArgumentParser()
parser.add_argument("--data_path", type=str, default=None)
parser.add_argument("--output_path", type=str, default=None)
parser.add_argument("--max_length", type=int, default=2048)
parser.add_argument("--batch_size_per_device", type=int, default=4)
parser.add_argument("--conv_template", type=str, default="vicuna_v1.1")
parser.add_argument("--use_flash_attention", type=bool, default=False)
parser.add_argument("--only_answer", type=bool, default=False)
parser.add_argument("--random_shuffle", type=bool, default=False)
parser.add_argument("--model_name_or_path", type=str, default="mistralai/Mistral-7B-v0.1")

args = parser.parse_args()

embed_pipeline = Pipeline("embed_pipeline", 
                          data_path = args.data_path,   # json file with sharegpt format
                          output_path = args.output_path,  # output path (pickle format)
                          model_name_or_path = args.model_name_or_path,  # model name or path e.g. mistralai/Mistral-7B-v0.1
                          max_length = args.max_length,
                          use_flash_attention = args.use_flash_attention,  
                          batch_size_per_device = args.batch_size_per_device,
                          conv_template = args.conv_template,
                          only_answer = args.only_answer,
                          random_shuffle = args.random_shuffle,
                          bfloat16 = True
                          )

embed_pipeline.run()